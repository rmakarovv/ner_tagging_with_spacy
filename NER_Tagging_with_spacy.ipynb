{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "    author: Roman Makarov\n",
        "    e-mail: mcronomus@gmail.com"
      ],
      "metadata": {
        "id": "G9pCPe_19wTF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUxw96in7Lej"
      },
      "source": [
        "#Downloading and preprocessing training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hSHD3k-qpJo"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8StZCLDIGOiS",
        "outputId": "1c1dc522-7b22-4bf0-9d1f-917c9f95e90f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1fBbJhp7ZkaJWXCdV4XwJRAtdrG4J3BUT\n",
            "To: /content/data.zip\n",
            "\r  0% 0.00/3.83M [00:00<?, ?B/s]\r100% 3.83M/3.83M [00:00<00:00, 224MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Downloading training and testing data from drive\n",
        "!gdown 1fBbJhp7ZkaJWXCdV4XwJRAtdrG4J3BUT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMniJjgOKeqP"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!unzip data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flcsKdTSKi43"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7HoueykPJBF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "dir_name = 'train_data'\n",
        "\n",
        "data_folders = [x[0] for x in os.walk(dir_name)]\n",
        "data_folders = data_folders[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dToOAlEVHGpB"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# A function to read the text and annotation files\n",
        "def read_data(data_dir, file_id):\n",
        "    with open(os.path.join(data_dir, f'{file_id}.txt'), 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    annotations = []\n",
        "    with open(os.path.join(data_dir, f'{file_id}.ann'), 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.startswith('T'):\n",
        "                annotation = line.strip().split('\\t')[1:]\n",
        "\n",
        "                # Splitting and saving data\n",
        "                try:\n",
        "                    annotation_type, start, end = annotation[0].split()\n",
        "                    start = int(start)\n",
        "                    end = int(end)\n",
        "                    annotations.append((annotation_type, start, end))\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "    return text, annotations\n",
        "\n",
        "# Load the dataset to data list\n",
        "data = []\n",
        "for data_dir in data_folders:\n",
        "    file_ids = sorted([filename.split('.')[0] for filename in os.listdir(data_dir) if filename.endswith('.txt')])\n",
        "\n",
        "    for file_id in file_ids:\n",
        "        text, annotations = read_data(data_dir, file_id)\n",
        "\n",
        "        labels = []\n",
        "        for annotation in annotations:\n",
        "            label = annotation[0]\n",
        "            start = int(annotation[1])\n",
        "            end = int(annotation[2])\n",
        "\n",
        "            # excluding intersections\n",
        "            intersect = False\n",
        "            for label_ in labels:\n",
        "                if label_[0] <= start <= label_[1] or label_[0] <= end <= label_[1]:\n",
        "                    intersect = True\n",
        "                    break\n",
        "\n",
        "            if intersect:\n",
        "                continue\n",
        "\n",
        "            labels.append((start, end, label, text[start:end]))\n",
        "\n",
        "        # Checking if some tag is labeled twice and excluding it in this case\n",
        "        new_labels = []\n",
        "        for i in range(len(labels)):\n",
        "            good = True\n",
        "            for j in range(i + 1, len(labels)):\n",
        "                if labels[i][3] in labels[j][3]:\n",
        "                    good = False\n",
        "                    break\n",
        "            if good:\n",
        "                new_labels.append(labels[i][:3])\n",
        "        labels = new_labels\n",
        "\n",
        "        data.append((text, {'entities': sorted(labels, key=lambda x: x[0])}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XI58G5Pt5pw",
        "outputId": "944d834e-f271-4042-e0c2-b7ab72cd6a39"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "841"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparing spacy model"
      ],
      "metadata": {
        "id": "Mehrq0lI5tUs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1qppqN-s-hQ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!python -m spacy download ru_core_news_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uo5_IvnSs-hR"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"ru_core_news_lg\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1MSMwpbMi7QAbUzqtq8A1_aUppL-ke7GY"
      ],
      "metadata": {
        "id": "lLkskaeUNeWi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5def861a-4c63-40d2-b69a-77851dc912ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1MSMwpbMi7QAbUzqtq8A1_aUppL-ke7GY\n",
            "To: /content/base_config.cfg\n",
            "\r  0% 0.00/1.80k [00:00<?, ?B/s]\r100% 1.80k/1.80k [00:00<00:00, 2.79MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0FAAvkFtZMB",
        "outputId": "ce9fdaa9-6c68-4ac2-bbdc-ff8eb17de241"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-14 06:40:07.444500: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-14 06:40:08.560102: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy init fill-config base_config.cfg config.cfg"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Converting training and testing data to train.spacy and dev.spacy files\n",
        "\n",
        "*Reference: https://spacy.io/usage/training*"
      ],
      "metadata": {
        "id": "4FMcHHGE5zsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data_train, data_test = train_test_split(data, test_size=0.2, shuffle=True)"
      ],
      "metadata": {
        "id": "xiECd6V--KCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHtubHDWrY33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c9532e7-a554-42ce-c47c-4c3b92b585f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 672/672 [00:03<00:00, 207.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped: 64 entities\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from spacy.tokens import DocBin\n",
        "\n",
        "db = DocBin()\n",
        "num_skipped = 0\n",
        "\n",
        "for text, annot in tqdm(data_train):\n",
        "    doc = nlp.make_doc(text)\n",
        "    ents = []\n",
        "    for start, end, label in annot[\"entities\"]:\n",
        "        # contract\n",
        "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "        if span is None:\n",
        "            num_skipped += 1\n",
        "            # print(\"Skipping entity\")\n",
        "        else:\n",
        "            ents.append(span)\n",
        "\n",
        "    doc.ents = ents\n",
        "    db.add(doc)\n",
        "\n",
        "print(f'Skipped: {num_skipped} entities')\n",
        "db.to_disk(\"./train.spacy\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from spacy.tokens import DocBin\n",
        "\n",
        "db = DocBin()\n",
        "num_skipped = 0\n",
        "\n",
        "for text, annot in tqdm(data_test):\n",
        "    doc = nlp.make_doc(text)\n",
        "    ents = []\n",
        "    for start, end, label in annot[\"entities\"]:\n",
        "        # contract\n",
        "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "        if span is None:\n",
        "            num_skipped += 1\n",
        "            # print(\"Skipping entity\")\n",
        "        else:\n",
        "            ents.append(span)\n",
        "\n",
        "    doc.ents = ents\n",
        "    db.add(doc)\n",
        "\n",
        "print(f'Skipped: {num_skipped} entities')\n",
        "db.to_disk(\"./dev.spacy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHBo3B53-UlB",
        "outputId": "2c248e40-152d-4207-ea4c-36c80aa89093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169/169 [00:00<00:00, 240.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped: 17 entities\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Staring training process with initialized config and training data"
      ],
      "metadata": {
        "id": "w5Ol_ecH6DyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./dev.spacy --gpu-id 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bny7mpHn_DyK",
        "outputId": "6f236ff4-980f-4fa4-c3e6-46f2417143d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-14 04:23:20.714720: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2023-04-14 04:23:28,161] [INFO] Set up nlp object from config\n",
            "[2023-04-14 04:23:28,177] [INFO] Pipeline: ['tok2vec', 'ner']\n",
            "[2023-04-14 04:23:28,183] [INFO] Created vocabulary\n",
            "[2023-04-14 04:23:28,184] [INFO] Finished initializing nlp object\n",
            "[2023-04-14 04:23:36,562] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "  0       0          0.00    121.67    0.00    0.00    0.00    0.00\n",
            "  0     200       2188.53  14585.28   27.82   47.59   19.66    0.28\n",
            "  0     400        463.43  10835.43   27.71   39.40   21.36    0.28\n",
            "  0     600       1510.30  10774.82   38.69   53.89   30.18    0.39\n",
            "  1     800        965.99  10081.80   46.37   53.47   40.93    0.46\n",
            "  1    1000        811.18   9442.59   48.09   54.50   43.03    0.48\n",
            "  1    1200        900.85   9093.30   45.56   54.68   39.04    0.46\n",
            "  2    1400        625.52   8081.51   49.11   55.43   44.08    0.49\n",
            "  2    1600        575.07   7739.81   47.44   56.85   40.70    0.47\n",
            "  2    1800        525.23   7424.02   47.90   55.23   42.29    0.48\n",
            "  2    2000        666.55   8181.45   48.86   56.23   43.20    0.49\n",
            "  3    2200        697.46   6738.47   51.04   55.36   47.34    0.51\n",
            "  3    2400        636.97   6818.79   51.69   54.10   49.49    0.52\n",
            "  3    2600        529.71   6378.30   51.88   55.86   48.44    0.52\n",
            "  4    2800        641.51   6117.33   50.13   58.54   43.83    0.50\n",
            "  4    3000        670.79   5399.59   51.39   54.97   48.26    0.51\n",
            "  4    3200        776.60   6458.66   51.70   54.09   49.51    0.52\n",
            "  5    3400        708.66   5719.64   52.42   58.16   47.71    0.52\n",
            "  5    3600        718.77   4827.26   52.84   54.97   50.86    0.53\n",
            "  5    3800        749.24   4904.27   51.53   55.32   48.23    0.52\n",
            "  5    4000       1126.67   5554.06   52.29   59.18   46.84    0.52\n",
            "  6    4200        726.74   4385.28   52.53   56.68   48.94    0.53\n",
            "  6    4400        831.48   4482.68   52.60   56.58   49.14    0.53\n",
            "  6    4600       6822.43   4914.20   51.55   56.42   47.45    0.52\n",
            "  7    4800       5734.34   4610.43   51.82   53.69   50.08    0.52\n",
            "  7    5000       1065.75   4018.19   53.10   56.43   50.14    0.53\n",
            "  7    5200        969.84   4191.99   53.16   53.40   52.92    0.53\n",
            "  8    5400       1133.21   4260.23   54.41   55.86   53.02    0.54\n",
            "  8    5600        923.88   3664.53   53.76   53.37   54.15    0.54\n",
            "  8    5800       1293.70   3934.60   54.48   54.98   54.00    0.54\n",
            "  9    6000       1125.53   4467.41   54.62   54.12   55.14    0.55\n",
            "  9    6200       1086.12   3966.67   53.26   55.99   50.78    0.53\n",
            " 10    6400       1516.52   5411.27   54.31   54.36   54.26    0.54\n",
            " 10    6600       2444.92   5128.89   55.03   56.05   54.06    0.55\n",
            " 11    6800       1757.35   5327.84   55.63   56.33   54.96    0.56\n",
            " 12    7000       1562.46   4734.57   54.80   57.10   52.67    0.55\n",
            " 13    7200       1623.88   4112.77   55.90   57.37   54.50    0.56\n",
            " 14    7400       1565.12   3734.15   54.60   57.73   51.79    0.55\n",
            " 15    7600       1607.33   3619.15   54.57   57.73   51.74    0.55\n",
            " 16    7800       1483.47   3300.26   54.75   56.88   52.78    0.55\n",
            " 17    8000       1681.94   3247.84   54.94   57.71   52.43    0.55\n",
            " 18    8200       1432.25   2992.64   54.62   57.18   52.28    0.55\n",
            " 18    8400       1566.33   2952.80   54.83   55.00   54.67    0.55\n",
            " 19    8600       1781.70   2965.23   54.42   55.50   53.37    0.54\n",
            " 20    8800       1285.34   2497.15   54.93   56.00   53.91    0.55\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "output/model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Checking how the model trained"
      ],
      "metadata": {
        "id": "D0pOukLI6Ji-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFzuaigXxKXH"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"./output/model-best\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3Rs2kyDxKXI",
        "outputId": "f27da240-9ee1-46f8-f8c7-43a75d61dd92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROFESSION 0 24 for Глава департамента ЦБ РФ\n",
            "PERSON 25 40 for Надежда Иванова\n",
            "CITY 67 74 for Иванова\n",
            "DATE 84 91 for 13 июня\n",
            "AGE 104 110 for 60 лет\n",
            "ORGANIZATION 149 151 for ЦБ\n",
            "DATE 212 223 for с 1995 года\n",
            "ORGANIZATION 232 253 for Центрального банка РФ\n",
            "PERSON 321 336 for Надежда Иванова\n",
            "EVENT 337 346 for назначена\n",
            "PROFESSION 366 393 for заместителя председателя ЦБ\n",
            "DATE 403 412 for в четверг\n",
            "PERSON 427 434 for Иванова\n",
            "DATE 446 453 for 13 июня\n",
            "AGE 458 467 for 60-летний\n",
            "ORGANIZATION 495 507 for Банка России\n",
            "PERSON 517 530 for Госбанка СССР\n",
            "DATE 563 574 for с 1975 года\n",
            "NUMBER 622 630 for почти 20\n",
            "DATE 637 648 for с 1995 года\n",
            "PERSON 651 658 for Иванова\n",
            "ORGANIZATION 668 696 for совет директоров Центробанка\n",
            "ORGANIZATION 754 786 for департаменте банковского надзора\n",
            "ORDINAL 870 876 for первое\n",
            "EVENT 877 887 for назначение\n",
            "PROFESSION 895 920 for председателя Банка России\n",
            "PERSON 921 940 for Эльвиры Набиуллиной\n",
            "PERSON 977 988 for Центробанка\n",
            "ORDINAL 1043 1050 for первого\n",
            "ORGANIZATION 1060 1101 for ЦБ по вопросам денежно-кредитной политики\n",
            "PERSON 1102 1116 for Ксении Юдаевой\n",
            "PROFESSION 1154 1164 for президента\n",
            "PERSON 1193 1208 for Алексей Улюкаев\n",
            "DATE 1218 1231 for в понедельник\n",
            "ORGANIZATION 1251 1268 for Минэкономразвития\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(\"\"\"Глава департамента ЦБ РФ Надежда Иванова получила статус зампреда\n",
        "\n",
        "Иванова, которой 13 июня исполнилось 60 лет, всю свою жизнь проработала в системе ЦБ. Сводный экономический департамент Банка России возглавляет с 1995 года.\n",
        "Здание Центрального банка РФ. Архив\n",
        "\n",
        "Директор сводного экономического департамента Банка России Надежда Иванова назначена также на должность заместителя председателя ЦБ, сообщил в четверг регулятор.\n",
        "\n",
        "\n",
        "\n",
        "Иванова, у которой 13 июня был 60-летний юбилей, работает в системе Банка России (ранее — Госбанка СССР) с окончания института, то есть с 1975 года. Сводный экономический департамент возглавляет почти 20 лет — с 1995 года.\n",
        "\n",
        "Иванова входит в совет директоров Центробанка. До сводного экономического департамента она трудилась в департаменте банковского надзора.\n",
        "\n",
        "Сводный экономический департамент входит в блок денежно-кредитной политики.\n",
        "\n",
        "Это первое назначение нового председателя Банка России Эльвиры Набиуллиной на этом посту. Раньше в руководстве Центробанка преобладали мужчины. Эксперты ждут назначения на пост первого зампреда ЦБ по вопросам денежно-кредитной политики Ксении Юдаевой, возглавляющей экспертное управление президента РФ. Ранее этот пост занимал Алексей Улюкаев, который в понедельник стал руководителем Минэкономразвития.\"\"\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(f'{ent.label_} {ent.start_char} {ent.end_char} for {ent}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Predicting tags for test data"
      ],
      "metadata": {
        "id": "He1VDdrZN_1G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-xs9WVN1E1X"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "\n",
        "def spacy(model, out_file='answer.txt', test_dir='test_data'):\n",
        "    result_string = ''\n",
        "\n",
        "    for filename in os.listdir(test_dir):\n",
        "        file_path = os.path.join(test_dir, filename)\n",
        "        result_list = []\n",
        "\n",
        "        result_string += f'{filename[:-4]}.ann\\n'\n",
        "\n",
        "        # Reading the text and converting it to lower case\n",
        "        file_text = ''\n",
        "        with open(file_path, 'rb') as f:\n",
        "            file_text = f.read().decode(errors='replace')\n",
        "\n",
        "        doc = model(file_text)\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            result_string += f'{ent.label_} {ent.start_char} {ent.end_char}\\n'\n",
        "            # print(f'{ent.label_} {ent.start_char} {ent.end_char} for {ent}')\n",
        "\n",
        "    with open(out_file, 'w') as f:\n",
        "        f.write(result_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pm6yB77S1KvR"
      },
      "outputs": [],
      "source": [
        "spacy(nlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Saving the model parameters for future usage"
      ],
      "metadata": {
        "id": "XTRAWFleN8UD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-E_5vXg5QFs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbb0fdb8-7ec9-4047-b4fc-aae01b293a44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/output/ (stored 0%)\n",
            "  adding: content/output/model-best/ (stored 0%)\n",
            "  adding: content/output/model-best/meta.json (deflated 71%)\n",
            "  adding: content/output/model-best/vocab/ (stored 0%)\n",
            "  adding: content/output/model-best/vocab/strings.json (deflated 90%)\n",
            "  adding: content/output/model-best/vocab/key2row (stored 0%)\n",
            "  adding: content/output/model-best/vocab/lookups.bin (stored 0%)\n",
            "  adding: content/output/model-best/vocab/vectors (deflated 45%)\n",
            "  adding: content/output/model-best/vocab/vectors.cfg (stored 0%)\n",
            "  adding: content/output/model-best/ner/ (stored 0%)\n",
            "  adding: content/output/model-best/ner/moves (deflated 80%)\n",
            "  adding: content/output/model-best/ner/model (deflated 8%)\n",
            "  adding: content/output/model-best/ner/cfg (deflated 33%)\n",
            "  adding: content/output/model-best/tokenizer (deflated 84%)\n",
            "  adding: content/output/model-best/config.cfg (deflated 60%)\n",
            "  adding: content/output/model-best/tok2vec/ (stored 0%)\n",
            "  adding: content/output/model-best/tok2vec/model (deflated 7%)\n",
            "  adding: content/output/model-best/tok2vec/cfg (stored 0%)\n",
            "  adding: content/output/model-last/ (stored 0%)\n",
            "  adding: content/output/model-last/meta.json (deflated 71%)\n",
            "  adding: content/output/model-last/vocab/ (stored 0%)\n",
            "  adding: content/output/model-last/vocab/strings.json (deflated 90%)\n",
            "  adding: content/output/model-last/vocab/key2row (stored 0%)\n",
            "  adding: content/output/model-last/vocab/lookups.bin (stored 0%)\n",
            "  adding: content/output/model-last/vocab/vectors (deflated 45%)\n",
            "  adding: content/output/model-last/vocab/vectors.cfg (stored 0%)\n",
            "  adding: content/output/model-last/ner/ (stored 0%)\n",
            "  adding: content/output/model-last/ner/moves (deflated 80%)\n",
            "  adding: content/output/model-last/ner/model (deflated 8%)\n",
            "  adding: content/output/model-last/ner/cfg (deflated 33%)\n",
            "  adding: content/output/model-last/tokenizer (deflated 84%)\n",
            "  adding: content/output/model-last/config.cfg (deflated 60%)\n",
            "  adding: content/output/model-last/tok2vec/ (stored 0%)\n",
            "  adding: content/output/model-last/tok2vec/model (deflated 7%)\n",
            "  adding: content/output/model-last/tok2vec/cfg (stored 0%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r output.zip /content/output"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}